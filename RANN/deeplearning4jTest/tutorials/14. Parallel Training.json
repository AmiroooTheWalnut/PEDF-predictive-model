{"paragraphs":[{"text":"%md\n### Note\n\nPlease view the [README](https://github.com/eclipse/deeplearning4j-examples/blob/master/tutorials/README.md) to learn about installing, setting up dependencies, and importing notebooks in Zeppelin","dateUpdated":"2018-02-26T03:58:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Note</h3>\n<p>Please view the <a href=\"https://github.com/eclipse/deeplearning4j-examples/blob/master/tutorials/README.md\">README</a> to learn about installing, setting up dependencies, and importing notebooks in Zeppelin</p>\n"}]},"apps":[],"jobName":"paragraph_1519616666249_-322338450","id":"20171114-182247_1512945038","dateCreated":"2018-02-26T03:44:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6559","user":"admin","dateFinished":"2018-02-26T03:58:45+0000","dateStarted":"2018-02-26T03:58:45+0000"},{"text":"%md\n### Background\n\nTraining neural network models can be a computationally expensive task.  In order to speed up the training process, you can choose to train your models in parallel with multiple GPU's if they are installed on your machine. With deeplearning4j (DL4J), this isn't a difficult thing to do. In this tutorial we will use the MNIST dataset (dataset of handwritten images) to train a feed forward neural network in parallel with distributed GPU's. \n\nFirst you must update your pom.xml file if its configured to use CPU's by default. The last line of the following\n\n```\n<name>DeepLearning4j Examples Parent</name>\n<description>Examples of training different data sets</description>\n<properties>\n<nd4j.backend>nd4j-native-platform</nd4j.backend>\n```\n\nshould be changed to \n\n```\n<nd4j.backend>nd4j-cuda-8.0-platform</<nd4j.backend>\n```","dateUpdated":"2018-02-26T03:58:47+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Background</h3>\n<p>Training neural network models can be a computationally expensive task.  In order to speed up the training process, you can choose to train your models in parallel with multiple GPU's if they are installed on your machine. With deeplearning4j (DL4J), this isn't a difficult thing to do. In this tutorial we will use the MNIST dataset (dataset of handwritten images) to train a feed forward neural network in parallel with distributed GPU's.</p>\n<p>First you must update your pom.xml file if its configured to use CPU's by default. The last line of the following</p>\n<pre><code>&lt;name&gt;DeepLearning4j Examples Parent&lt;/name&gt;\n&lt;description&gt;Examples of training different data sets&lt;/description&gt;\n&lt;properties&gt;\n&lt;nd4j.backend&gt;nd4j-native-platform&lt;/nd4j.backend&gt;\n</code></pre>\n<p>should be changed to</p>\n"}]},"apps":[],"jobName":"paragraph_1519616666249_-322338450","id":"20171114-182306_1904777643","dateCreated":"2018-02-26T03:44:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6560","user":"admin","dateFinished":"2018-02-26T03:58:47+0000","dateStarted":"2018-02-26T03:58:47+0000"},{"text":"%md\n### Imports","dateUpdated":"2018-02-26T03:58:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Imports</h3>\n"}]},"apps":[],"jobName":"paragraph_1519616666249_-322338450","id":"20171114-182305_1521177108","dateCreated":"2018-02-26T03:44:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6561","user":"admin","dateFinished":"2018-02-26T03:58:40+0000","dateStarted":"2018-02-26T03:58:40+0000"},{"text":"%spark.dep\nz.load(\"org.deeplearning4j:deeplearning4j-parallel-wrapper_2.10:0.9.1\")","user":"admin","dateUpdated":"2018-02-26T03:49:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@97354aa\n"}]},"apps":[],"jobName":"paragraph_1519616691663_-1311881973","id":"20180226-034451_1473422923","dateCreated":"2018-02-26T03:44:51+0000","dateStarted":"2018-02-26T03:49:35+0000","dateFinished":"2018-02-26T03:49:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6562"},{"text":"import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator;\nimport org.deeplearning4j.eval.Evaluation;\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm;\nimport org.deeplearning4j.nn.conf.MultiLayerConfiguration;\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration;\nimport org.deeplearning4j.nn.conf.Updater;\nimport org.deeplearning4j.nn.conf.inputs.InputType;\nimport org.deeplearning4j.nn.conf.layers.ConvolutionLayer;\nimport org.deeplearning4j.nn.conf.layers.DenseLayer;\nimport org.deeplearning4j.nn.conf.layers.OutputLayer;\nimport org.deeplearning4j.nn.conf.layers.SubsamplingLayer;\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork;\nimport org.deeplearning4j.nn.weights.WeightInit;\nimport org.deeplearning4j.optimize.listeners.ScoreIterationListener;\nimport org.nd4j.linalg.activations.Activation;\nimport org.nd4j.linalg.api.buffer.DataBuffer;\nimport org.nd4j.linalg.api.buffer.util.DataTypeUtil;\nimport org.nd4j.linalg.api.ndarray.INDArray;\nimport org.nd4j.linalg.dataset.DataSet;\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator;\nimport org.nd4j.linalg.lossfunctions.LossFunctions;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.deeplearning4j.parallelism.ParallelWrapper;","user":"admin","dateUpdated":"2018-02-26T03:51:46+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.eval.Evaluation\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.MultiLayerConfiguration\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.Updater\nimport org.deeplearning4j.nn.conf.inputs.InputType\nimport org.deeplearning4j.nn.conf.layers.ConvolutionLayer\nimport org.deeplearning4j.nn.conf.layers.DenseLayer\nimport org.deeplearning4j.nn.conf.layers.OutputLayer\nimport org.deeplearning4j.nn.conf.layers.SubsamplingLayer\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.deeplearning4j.optimize.listeners.ScoreIterationListener\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.api.buffer.DataBuffer\nimport org.nd4j.linalg.api.buffer.util.DataTypeUtil\nimport org.nd4j.linalg.api.ndarray.INDArray\nimport org.nd4j.linalg.dataset.DataSet\nimport org.nd4j.linalg.dataset.api.iterator.DataSetIterator\nimport org.nd4j.linalg.lossfunctions.LossFunctions\nimport org.slf4j.Logger\nimport org.slf4j.LoggerFactory\nimport org.deeplearning4j.parallelism.ParallelWrapper\n"}]},"apps":[],"jobName":"paragraph_1519616666249_-322338450","id":"20171114-182236_1794315277","dateCreated":"2018-02-26T03:44:26+0000","dateStarted":"2018-02-26T03:51:46+0000","dateFinished":"2018-02-26T03:51:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6563"},{"text":"%md\nTo obtain the data, we use built-in DataSetIterators for the MNIST with a random seed of 12345. These DataSetIterators can be used to directly feed the data into a neural network.","dateUpdated":"2018-02-26T03:58:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>To obtain the data, we use built-in DataSetIterators for the MNIST with a random seed of 12345. These DataSetIterators can be used to directly feed the data into a neural network.</p>\n"}]},"apps":[],"jobName":"paragraph_1519616666250_-321184204","id":"20171114-191417_477411559","dateCreated":"2018-02-26T03:44:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6564","user":"admin","dateFinished":"2018-02-26T03:58:36+0000","dateStarted":"2018-02-26T03:58:35+0000"},{"text":"\nval batchSize = 128\nval mnistTrain = new MnistDataSetIterator(batchSize,true,12345)\nval mnistTest = new MnistDataSetIterator(batchSize,false,12345)","user":"admin","dateUpdated":"2018-02-26T03:52:02+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"batchSize: Int = 128\nmnistTrain: org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator@6dd3039d\nmnistTest: org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator@4b4de1f\n"}]},"apps":[],"jobName":"paragraph_1519616666250_-321184204","id":"20171117-183822_417150439","dateCreated":"2018-02-26T03:44:26+0000","dateStarted":"2018-02-26T03:52:02+0000","dateFinished":"2018-02-26T03:52:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6565"},{"text":"%md\n\nNext, we set up the neural network configuration using a convolutional configuration and initialize the model. ","dateUpdated":"2018-02-26T03:58:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next, we set up the neural network configuration using a convolutional configuration and initialize the model.</p>\n"}]},"apps":[],"jobName":"paragraph_1519616666250_-321184204","id":"20171117-183924_1369734651","dateCreated":"2018-02-26T03:44:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6566","user":"admin","dateFinished":"2018-02-26T03:58:34+0000","dateStarted":"2018-02-26T03:58:34+0000"},{"text":"val nChannels = 1\nval outputNum = 10\nval seed = 123\n\nval conf = new NeuralNetConfiguration.Builder()\n            .seed(seed)\n            .weightInit(WeightInit.XAVIER)\n            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n            .updater(Updater.NESTEROVS)\n            .list()\n            .layer(0, new ConvolutionLayer.Builder(5, 5)\n                //nIn and nOut specify depth. nIn here is the nChannels and nOut is the number of filters to be applied\n                .nIn(nChannels)\n                .stride(1, 1)\n                .nOut(20)\n                .activation(Activation.IDENTITY)\n                .build())\n            .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n                .kernelSize(2,2)\n                .stride(2,2)\n                .build())\n            .layer(2, new ConvolutionLayer.Builder(5, 5)\n                //Note that nIn need not be specified in later layers\n                .stride(1, 1)\n                .nOut(50)\n                .activation(Activation.IDENTITY)\n                .build())\n            .layer(3, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n                .kernelSize(2,2)\n                .stride(2,2)\n                .build())\n            .layer(4, new DenseLayer.Builder().activation(Activation.RELU)\n                .nOut(500).build())\n            .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n                .nOut(outputNum)\n                .activation(Activation.SOFTMAX)\n                .build())\n            .setInputType(InputType.convolutionalFlat(28,28,1)) //See note below\n            .backprop(true).pretrain(false).build()\n\nval model = new MultiLayerNetwork(conf)\nmodel.init()","user":"admin","dateUpdated":"2018-02-26T03:52:27+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"nChannels: Int = 1\noutputNum: Int = 10\nseed: Int = 123\nwarning: there were 1 deprecation warning(s); re-run with -deprecation for details\nconf: org.deeplearning4j.nn.conf.MultiLayerConfiguration = \n{\n  \"backprop\" : true,\n  \"backpropType\" : \"Standard\",\n  \"cacheMode\" : \"NONE\",\n  \"confs\" : [ {\n    \"cacheMode\" : \"NONE\",\n    \"epochCount\" : 0,\n    \"iterationCount\" : 0,\n    \"l1ByParam\" : { },\n    \"l2ByParam\" : { },\n    \"layer\" : {\n      \"convolution\" : {\n        \"activationFn\" : {\n          \"Identity\" : { }\n        },\n        \"biasInit\" : 0.0,\n        \"biasUpdater\" : null,\n        \"constraints\" : null,\n        \"convolutionMode\" : \"Truncate\",\n        \"cudnnAlgoMode\" : \"PREFER_FASTEST\",\n        \"cudnnBwdDataAlgo\" : null,\n        \"cudnnBwdFilterAlgo\" : null,\n        \"cudnnFwdAlgo\" : null,\n        \"dilation\" : [ 1, 1 ],\n        \"dist\" : null,\n        \"gradientNormalization\" : \"None\",\n        \"gradientNormalizationThreshold\" : 1.0,\n ...model: org.deeplearning4j.nn.multilayer.MultiLayerNetwork = org.deeplearning4j.nn.multilayer.MultiLayerNetwork@7efae1eb\n"}]},"apps":[],"jobName":"paragraph_1519616666250_-321184204","id":"20171117-183938_535180830","dateCreated":"2018-02-26T03:44:26+0000","dateStarted":"2018-02-26T03:52:27+0000","dateFinished":"2018-02-26T03:52:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6567"},{"text":"%md\nNext we need to configure the parallel training with the ParallelWrapper class using the MultiLayerNetwork as the input.  The ParallelWrapper will take care of load balancing between different GPUs. \n\nThe notion is that the model will be duplicated within the ParallelWrapper. The prespecified number of workers (in this case 2) will then train its own model using its data. After a specified number of iterations (in this case 3), all models will be averaged and workers will receive duplicate models. The training process will then continue in this way until the model is fully trained. \n\n","dateUpdated":"2018-02-26T03:58:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Next we need to configure the parallel training with the ParallelWrapper class using the MultiLayerNetwork as the input.  The ParallelWrapper will take care of load balancing between different GPUs.</p>\n<p>The notion is that the model will be duplicated within the ParallelWrapper. The prespecified number of workers (in this case 2) will then train its own model using its data. After a specified number of iterations (in this case 3), all models will be averaged and workers will receive duplicate models. The training process will then continue in this way until the model is fully trained.</p>\n"}]},"apps":[],"jobName":"paragraph_1519616666250_-321184204","id":"20171117-184111_186597875","dateCreated":"2018-02-26T03:44:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6568","user":"admin","dateFinished":"2018-02-26T03:58:32+0000","dateStarted":"2018-02-26T03:58:32+0000"},{"text":" val wrapper = new ParallelWrapper.Builder(model)\n            .prefetchBuffer(24)\n            .workers(2)\n            .averagingFrequency(3)\n            .reportScoreAfterAveraging(true)\n            .build()","user":"admin","dateUpdated":"2018-02-26T03:53:02+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"wrapper: org.deeplearning4j.parallelism.ParallelWrapper = ParallelWrapper(uuid=69921c2e-9aec-4698-9714-a88a8649885d, model=org.deeplearning4j.nn.multilayer.MultiLayerNetwork@7efae1eb, workers=2, prefetchSize=24, averagingFrequency=3, zoo=null, trainerContext=org.deeplearning4j.parallelism.factory.DefaultTrainerContext@39167d7a, iterationsCounter=0, reportScore=true, averageUpdaters=true, legacyAveraging=true, wasAveraged=false, stopFit=false, listeners=[], storageRouter=null, isMQ=false, workspaceMode=SEPARATE, trainerContextArgs=null, debug=false, executorService=java.util.concurrent.ThreadPoolExecutor@749057d[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0], workerCounter=0, gradientsAccumulator=null, handler=org.deeplearning4j.parallelism.ParallelWra..."}]},"apps":[],"jobName":"paragraph_1519616666250_-321184204","id":"20171117-184133_300800388","dateCreated":"2018-02-26T03:44:26+0000","dateStarted":"2018-02-26T03:53:02+0000","dateFinished":"2018-02-26T03:53:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6569"},{"text":"%md\n\nTo train the model, the fit method of the ParallelWrapper is used directly on the DataSetIterator. Because the ParallelWrapper class handles all the training details behind the scenes, it is very simple to parallelize this process using dl4j.","dateUpdated":"2018-02-26T03:58:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>To train the model, the fit method of the ParallelWrapper is used directly on the DataSetIterator. Because the ParallelWrapper class handles all the training details behind the scenes, it is very simple to parallelize this process using dl4j.</p>\n"}]},"apps":[],"jobName":"paragraph_1519616666251_-321568953","id":"20171117-184246_1945733739","dateCreated":"2018-02-26T03:44:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6570","user":"admin","dateFinished":"2018-02-26T03:58:29+0000","dateStarted":"2018-02-26T03:58:29+0000"},{"text":"wrapper.fit(mnistTrain)","user":"admin","dateUpdated":"2018-02-26T03:53:36+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1519616666251_-321568953","id":"20171117-185006_863656003","dateCreated":"2018-02-26T03:44:26+0000","dateStarted":"2018-02-26T03:53:36+0000","dateFinished":"2018-02-26T03:55:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:6571"},{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1519617414840_-1599091308","id":"20180226-035654_2005031284","dateCreated":"2018-02-26T03:56:54+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7574"}],"name":"Parallel","id":"2D93M89UM","angularObjects":{"2D9577PF1:existing_process":[],"2D86VT151:existing_process":[],"2D7PUMBZ1:existing_process":[],"2D7V8B4SM:existing_process":[],"2D6JWT1U6:existing_process":[],"2D8S4RU7Z:existing_process":[],"2D9YYJMG9:existing_process":[],"2D86WT5A7:existing_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}
